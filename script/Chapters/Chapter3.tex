% Chapter 1
% !TeX spellcheck = en_US 
\chapter{Neural networks} % Main chapter title

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter1} 
\setcounter{chapter}{3}
%\setcounter{section}{0}
In the \nameref{Chapter1} chapter, we breifly talked about a neural network which can be used 
to recognize hand-written digits. The neural network takes an image of a hand-written digit as 
an input and predicts a number which corresponds to the digit as an output. In this chapter, 
we will delve deeper into the architecture of neural networks and try to answer the following questions:
\begin{enumerate}
    \item What is the motivation behind using neural networks ?
    \item What is the idea behind the structure (neurons, input layer, hidden layers, output layer) 
    of neural networks ?
    \item Why do we need an activation function and a loss function ?
    \item How to train a neural network ?
    \item What are learning algorithms ?
    \item Why certain neural networks can approximate any continuous function defined on a compact set ?
\end{enumerate}
Recognition of handwritten digits is a classic problem for introducing the topic of neural networks. Therefore 
we will use this problem as an example to introduce various concepts related to neural networks.
  
%%%%%%%%%%%%%%%% Handwritten digit recog %%%%%%%%%
\section{Handwritten digit recognition}
Most people can easily recognize the handwritten digits shown in figure \ref{fig:my_digits}. Even if these digits were written 
in a different manner (for example refer figure \ref{fig:my_threes} ), one would still be able to recognize them. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\textwidth]{Figures/digits.png}
    \caption{One of the ways of writing digits from $1$ to $9$.}
    \label{fig:my_digits}
\end{figure} 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.3\textwidth]{Figures/three_digit.png}
    \caption{Different ways of writing the digit $3$.}
    \label{fig:my_threes}
\end{figure} This is due to the fact that our visual cortex has evolved over millions of years and therefore is magnificently 
adapted to understand the visual world \cite{vis_cortex}. The problem of recognizing hand-written digits is so trivial 
for our brains that we don't have to put an effort to recognize digits. But this task will immediately 
start to look complicated when one tries to write a computer program which can recognize hand-wriiten 
digits. 
Let us try to write a program for this task. We can start by coverting the digits into a pixel 
image ($28 \times 28$ for example) and assigning each pixel a value from $0$ to $1$ based upon the level of 
darkness/brightness (figure \ref{fig:pix7}). This can serve as an input to our program. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.35\textwidth]{Figures/pixel_image_7.png}
    \caption{A $28 \times 28$ pixel image of digit $7$. Each pixel is assigned a value from $0$ to $1$.}
    \label{fig:pix7}
\end{figure} 
We can identify distinct features of each digit and write code which looks for these features in the input image. 
Our program will therefore contain a lot of \emph{if} statements and \emph{for} loops to identify such features 
but more importantly will contain thousands of line of code to handle exceptions which arise from the fact that 
each digit can be written in a slightly different manner. It turns out that it is very difficult to express algorithmically 
the intuitions we have for recognizing digits. Soon we will realize the complexity of this task. What if we could write a 
program (for such fuzzy and difficult to-reason-about problems) that mimics the structure of our brain ?  
%%%%%%%%%%%%%%%% Intro to NN %%%%%%%%%%%%%
\section{Introduction to neural networks}
Neural networks tackles this problem in a different way. The construction of neural network
is inspired by the structure of our brain. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.5\textwidth]{Figures/artificial_vs_normal_neurons.png}
    \caption{Similarity between the structure of artificial and biological neuron \cite{bio_art_neuron}.}
    \label{fig:bio_neu}
\end{figure} 
The idea is to take a large number of handwritten digits together with labels and develop a system 
which can learn from these examples just like we learn. We leave it to the system to automatically infer rules 
from these examples and classify hand-written digits. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.5\textwidth]{Figures/training_examples_mnist.png}
    \caption{Some training examples from MNIST dataset \cite{lecun1998mnist}.}
    \label{fig:train_MNIST}
\end{figure} 

A neural network takes an input and gives an output. Figure \ref{fig:ff_net} shows a pictorial representation of
a neural network which can be used to classify handwritten digits. It has layers (input, hidden, output) which consist of 
artificial neurons. The neurons are connected. Each neuron stores a value (for example from $0$ to $1$ in 
our case of handwritten digit recognition). 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.8\textwidth]{Figures/ff_net.png}
    \caption{A neural network to classify handwritten digits \cite{nn_SVG}.}
    \label{fig:ff_net}
\end{figure} 
A neuron takes multiple inputs (say $a_1, a_2,....,a_n$) and gives the output $\sigma(\sum_{i =1}^n a_i w_i + b)$ where $\sigma$ is
called an activation function. The inputs are multiplied by weights, $w_i$ and added to a bias, $b$ before passing through the activation funciton.
The choice of activation function depends on the type of problem one is trying to solve. For example, for hand-written digit recognition one could use sigmoid/logistic function
as an activation function. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.4\textwidth]{Figures/neuron.png}
    \caption{A pictorial representation of an artificial neuron. It takes 
    multiple inputs (say $a_1, a_2,....,a_n$) and gives the output $\sigma(\sum_{i =1}^n a_i w_i + b)$.}
    \label{fig:neuron}
\end{figure} 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.4\textwidth]{Figures/sigmoid.png}
    \caption{A sigmoid/logistic activation function, $\sigma (z) = \frac{1}{1+ e^{-z}}$}
    \label{fig:sig}
\end{figure} 

Mathematically speaking, a neural network is simply a function that takes an input which passes through
its layers (made up of neurons which are mathematical functions) and results in an output. The neural 
network shown in figure \ref{fig:ff_net} has $784$ neurons in the input layer (containing data from a $28 \times 28$ pixel image), $15$
neurons in the hidden layer and $10$ neurons in the output layer. Each neuron in the output layer corresponds to a digit from $0$ to $9$; if
the input image is of digit $3$, only the fourth neuron in the outplayer will light up (output is close to $1$) while the rest will remain dormant (outputs are closer to $0$).
Such neural networks are called feed-forward neural networks. When we look at the architecture of a neural network, many questions can 
come to our minds, for example, 
\begin{enumerate}
    \item Why does the network has a particular structure (i.e. layers with neurons) ?
    \item What is the purpose of using an activation function ?
    \item How does a neural network able to recognize hand-written digits ?
\end{enumerate} 
Until now, it is also not clear that why this approach (of using neural networks) even works. Even if we assume that this approach works, another question arises,
which is, how do we get the weights, $w_i$ and biases $b_i$ of the network ? The answer to this question
comes from a key ability of neural networks which is \emph{learning}.
%%%%%%%%%%%%%%%% Learning from data %%%%%%%%%%%
\subsection{Learning from data}
Learning is the process of obtaining suitable weights and biases such that a neural network can perform
the desired task. There are several steps involved in learning. First step is to assign random values to 
weights and biases of the network and show the network some training examples. In the case of handwritten digit recognition
we use the MNIST dataset \cite{lecun1998mnist}. Since the network is initialized with random weights and biases it will wrongly classify handwritten digits. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.4\textwidth]{Figures/rand_inp.png}
    \caption{Results from a randomaly initialized neural network and how it compares to the desired output for a training example}
    \label{fig:randinp}
\end{figure} 
Next step is to tell the network to update its weights and biases in order to reduce the classfication error. This is done by introducing a 
cost/loss function which can quantify the classification error. We use a quadratic loss function, 
$$C(w,b) = \frac{1}{N} \sum_x {\|y(x) - a\|}^2$$
where $N$ denotes the number of training examples, $y(x)$ is the desired output corresponding to training example $x$, and 
$a$ is the neural network output. Our aim is to find $w,b$ such that the loss is minimum. The problem of learning from training data thus translates into
solving an optimization problem. But this process raises another set of questions,
\begin{enumerate}
    \item Which optimization algorithm to choose for minimizing the cost function?
    \item How to define a suitable loss function ?
    \item Does a global minimum exists for a chosen loss function ?
\end{enumerate}
To answer these questions and the questions that we raised earlier, we will have to first understand the origins of neural networks.
%%%%%%%%%%%%%% Origins of NN %%%%%%%%%%%%%%
\subsection{Origins of neural networks}
The artificial neurons we introduced earlier are called sigmoid neurons. In order to understand 
why sigmoid neurons are defined the way they are, we need to first understand another artificial 
neuron called perceptron. Perceptrons were developed in the $1950$s and $1960$s \cite{rosenblatt1958perceptron}. Unlike sigmoid neurons 
whose inputs, $x_i \in \mathbb{R}$, a perceptrons can take only binary inputs and produce a binary output.
The mathematical model of a perceptron is,
\begin{equation*}
    \text{output} = 
     \begin{cases}
       0 &\quad \text{if} \ \ \sum_i w_i x_i \leq \ \text{threshold} \\
       1 &\quad  \text{if} \ \ \sum_i w_i x_i > \ \text{threshold} 
     \end{cases}
\end{equation*}
where $x_i$ are the inputs and $w_i$ are the weights. The output is $1$ if $\sum_i w_i x_i$ exceeds a threshold 
value otherwise the output is $0$. Perceptrons can be used as a device to make simple decisions by weighing up evidence \cite{nielsen2015neural}. 
The following example illustrates how perceptrons can be used to make a decision.
\begin{boxedexample}
    Suppose a Rock band is coming to perform in your city. You like Rock music and you are trying to 
    decide whether or not to go to this event. You can make the decision by weighing up three factors:
   \begin{enumerate}
    \item Is the event nearby ?
    \item Are your friends accompanying you ?
    \item Is the weather good ?
   \end{enumerate}
   We can assign binary variables $x_1, x_2$ and $x_3$ to these three factors. For example, if the event is nearby we would
   have $x_1 =1$ but if it is far away we would have $x_1 = 0$. Similarly, if your friends are accompanying you, $x_2 =1$ otherwise $x_2 =0$.
   If the weather is good, $x_3 =1$ and if the weather is bad, $x_3 =0$. Now suppose, you absolutely love rock music and it doesn't really 
   matter to you if your friends can accompany you or if the event is nearby but you really loathe bad weather and you won't go if the
   weather is bad. You can use perceptrons to model this kind of decision-making. One way is to choose, $w_3 = 6$ for the weather and
   $w_1 =2, w_2 =2$ for other conditions. The larger value of $w_3$ in comparison to $w_1$ and $w_2$ suggests that weather matters to you a lot. 
   Finally, suppose you choose a thresold value of $5$ for the perceptron. With these choices, the perceptron mimics the desired 
   decision-making model, outputting $1$ whenever the weather is good, and $0$ whenever the weather is bad. By choosing different values for 
   weights and the thresold, we can get different models of decision making. 
\end{boxedexample}
It is obvious that the perceptron isn't a complete model of human descision-making. But it is plausible that if we use more layers in the network
with more number of perceptrons, the network could make more complex decisions:
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.4\textwidth]{Figures/perceptron_network.png}
    \caption{A network built out of perceptrons.}
    \label{fig:percep_net}
\end{figure} 
We can write the mathematical model of a perceptron in a way familiar to us, 
\begin{equation*}
    \text{output} = 
     \begin{cases}
       0 &\quad \text{if} \ \ \sum_i w_i x_i + b \leq 0 \\
       1 &\quad  \text{if} \ \ \sum_i w_i x_i + b > 0 
     \end{cases}
\end{equation*}
where perceptron's bias, $b \equiv -$threshold. We have seen that a network of perceptrons can be used as 
a method for weighing evidence to make decisions. We can also use perceptrons to compute the elementary logical functions such as
AND, OR, and NAND \cite{nielsen2015neural}. The NAND gates are universal for computation, and therefore it follows that perceptrons are also universal for
computation \cite{nielsen2015neural}. This property of perceptrons tells us that networks of perceptrons can be as powerful as any other computing device. These 
properties of perceptrons might make them an attractive option for solving decision-making problems but the property that we 
are really looking for in the network is the ability to tune its weights and biases in response to external stimuli (for example training data).
We want our network to learn from data. In order to devise learning algorithms for a network, we would like that if we make 
a small change in some weight (or bias) in the network, the output changes only by a small amount. Schematically, this is what we want:
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.4\textwidth]{Figures/precepnet2.png}
\end{figure}
 
Unfortunately, a network of perceptrons doesn't have this capability. This is the reason why we use sigmoid neurons. Unlike perceptrons, the sigmoid 
neurons can take input values between $0$ and $1$. The activation function used in sigmoid neurons can be described as, 
$$\sigma(z) = \frac{1}{1 + e^{-z}}.$$
The smoothness of $\sigma$ ensures that a small change in the input causes a small change in the output. 

Figure \ref{fig:deepNN} shows an architecture of a typical (feed-forward) neural network. It consists of an input layer, multiple hidden layers and an output layer. The choice of
the number of neurons in the input and output layers is governed by the problem one is trying to solve. For example, in the case of 
hand-wriiten digit recognition, if the input image consists of data from $28 \times 28 = 784$ pixels, the input layer will have $784$ neurons. We want to classify the 
images into digits from $0$ to $9$ and therefore the output layer will have $10$ neurons. The design (number of layers and number of neurons) of hidden layers
is based on heuristics. Let us now look at a neural network (in detail) which can classify handwritten digits. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.4\textwidth]{Figures/DeepNN.png}
    \caption{A feedforward neural network with an input layer $\in \mathbb{R}^{16}$, two hidden layers and an output layer $\in \mathbb{R}^2$ \cite{nn_SVG}.}
    \label{fig:deepNN}
\end{figure}
%%%%%%%%%%%% network to classify HD %%%%%%%%%%%%
\subsection{A network to classify handwritten digits}
We use a network consisting of an input layer $\in \mathbb{R}^{784}$, one hidden layer and an output layer $\in \mathbb{R}^{10}$ to classify handwritten
digits (see figure \ref{fig:NN_HD}). Let us try to understand how this neural network works. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.4\textwidth]{Figures/NN_2.png}
    \caption{A pictorial represenation of a neural network which can classify handwritten digits. It consists of an input layer $\in \mathbb{R}^{784}$ (only a few neurons are shown in the figure), one hidden layer and an output layer $\in \mathbb{R}^{10}$.}
    \label{fig:NN_HD}
\end{figure} 
The main feature of this network is its ability to learn from data. We use MNIST data set which consists of scanned images of digits written by $250$ people \cite{lecun1998mnist}. The MNIST data
comes in two parts; a training data set containing $60000$ imgaes and a test data set containing $10000$ images. The images are in grescale and have size of $28 \times 28$ pixels.
The MNIST data, along with hand-written digits, also provide correct labels corresponding to the digits. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.4\textwidth]{Figures/mnist_data_sample.png}
    \caption{A sample of training data from MNIST data set.}
    \label{fig:mnist_data}
\end{figure} 
The input $x$ to the network is a $28 \times 28 = 784$ dimensional vector and the output $a$ is a $10$ dimensional vector. In order to quantify, the 
accuracy of this network's prediction, we define a cost/loss function,
$$C(w,b) = \frac{1}{N} \sum_x {\|y(x) - a\|}^2$$
where $N$ denotes the number of training examples, $y(x)$ is the desired output (a $10$ dimensional vector) corresponding to a training example $x$, and 
$a$ is the neural network output. The idea is to find weights and biases which minimizes the difference between the desired output (labels) and the network prediction and we want this to happen for all training examples. For example, 
if an input $x$ is an image of digit $6$, we want our network's output $a$ to be as close as possible to $y(x) = (0,0,0,0,0,0,1,0,0,0)^T$. The problem of 
minimizing the loss function is an optimization problem. In the context of neural networks, the most popular methods which are used to minimize the loss function are 
inspired from the gradient descent method. 

Let us consider a simple example to understand the gradient descent method. Suppose we want to minimize a function $C(\mathbf{v})$ of two variables $v_1$ and $v_2$ (see figure \ref{fig:cost_f}).
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.4\textwidth]{Figures/cost_func.png}
    \caption{A function $C$ of two variables $v_1$ and $v_2$.}
    \label{fig:cost_f}
\end{figure} 
We can use Calculus to calculate the gradients of $C(\mathbf{v})$ with resprect to $v_1, v_2$ and use them to find places where $C$ is minimum. This method could work well when 
$C$ is a function of few variables but soon will turn into a nightmare when we increase the number of variables. Typically, the cost function of a neural network depends on a large number of
weights and biases; in fact the biggest neural networks consists of billions of weigths and biases. Therefore, one has to come up with cost effective 
methods of finding the minima of cost functions. Grandient descent is one such method. It is an interative method, where one starts 
with an initial guess for variables and update them over multiple iterations in such a 
way that the cost function reduces with each iteration. Consider the case of cost function $C(\mathbf{v}$). For small changes $\Delta v_1$ and  $\Delta v_2$ in variables $v_1$ and 
$v_2$ respectively, we can write the change in cost function $\Delta C$ as
\begin{equation}
    \label{eq:del_C}
    \Delta C \approx \frac{\partial C}{\partial v_1}\Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2 = \nabla C \cdot \Delta \mathbf{v}
\end{equation}
The aim is to find suitable change $\Delta \mathbf{v}$ which leads to reduction in $C$ i.e. $\Delta C < 0$. One such choice is $\Delta \mathbf{v} = -\eta \nabla C$ provided $\eta > 0$,
$$\Delta C \approx \nabla C \cdot \eta \nabla C = \eta \|\nabla C\|^2 < 0 $$
To conclude, in the gradient descent method we start with an initial guess $\mathbf{v}$ and update it by following
 $$\mathbf{v} \rightarrow \mathbf{v}' = \mathbf{v} - \eta \nabla C$$
untill we reach the minimum of $C(\mathbf{v})$. In the context of neural networks, a gradient descent update might look like this, 
\begin{equation}
    \begin{aligned}
        w_j \rightarrow w_j' = w_j - \eta \frac{\partial C}{\partial w_j}\\
        b_k \rightarrow b_k' = b_k - \eta \frac{\partial C}{\partial b_k}.
    \end{aligned}
\end{equation}
Note that the parameter $\eta$ should be sufficiently small in order to justify the assumption in equation \eqref{eq:del_C}. 
We call this parameter the \emph{learning rate}. The use of gradient descent method might seem as a
good alternative to using analytical methods but there are still a number of challenges in applying this method. Notice that the cost function has the form
$C = \frac{1}{N} \sum_x C_x$, that is, it's an average overs costs $C_x = \|y(x) - a\|^2$ for individual training examples. In order to make an update of gradient
descent, we need to compute the gradients $\nabla C_x$ separately for each training input and then average them, $\nabla C = \frac{1}{N} \sum_x \nabla C_x$. This update can take 
extremely long time to make when the number of training inputs are very large, thus making the \emph{learning} very slow. 

We can overcome the issue of slow \emph{learning} by using a variant of gradient descent method called \emph{stochastic gradient descent (SGD)} method. The idea is to estimate the gradient
$\nabla C$ by computing $\nabla C_x$ for a small sample of training examples and use it to make updates in weights and biases. The following steps are involved in using the stochastic gradient descent method,
\begin{enumerate}
    \item Shuffle the training inputs and divide into batches of size $m$. Each batch (we call them mini-batches) consists of training examples say, $X_1, X_2, .....X_m$.
    \item Calculate gradients for one mini-batch: $$\frac{1}{m} \sum_{i=1}^{m} \nabla C_{X_i} \approx \frac{1}{N} \sum_x \nabla C_x = \nabla C$$
    \item Make an update in the weights and biases: 
    $$w_j \rightarrow w_j' = w_j - \frac{\eta}{m} \sum_i \frac{\partial C_{X_i}}{\partial w_j}$$
    $$b_k \rightarrow b_k' = b_k - \frac{\eta}{m} \sum_i \frac{\partial C_{X_i}}{\partial b_k}$$
    \item Pick another mini-batch from the shuffled data and repeat steps $2$ and $3$ until all training inputs are exhausted. The end of this step is referred to as an \emph{epoch} of training.
    \item Repeat steps $1$ to $5$ until the network is sufficiently trained.
\end{enumerate}
The use of SGD significantly increases the speed at which a network is trained. We may be making less accurate updates (hence the name \emph{stochastic}) in the weights and biases in comparison to 
standard gradient descent method but nevertheless the updates are faster and over multiple iterations the cost (on an average) does go down. 
\section{Backpropagation}
Backpropagation is a fast algorithm for computing gradients \cite{nielsen2015neural}. Today, it is the workhorse of learning in neural networks. It uses chain-rule from 
Calculus to compute gradients $\frac{\partial C}{\partial w_i}, \frac{\partial C}{\partial b_i}$ and in the process reveals how changing the weights and 
biases changes the overall behaviour of the network. We will first see how backpropagation works for a very simple neural network and then generalize the procedure for
big neural networks \cite{backprop_grant}. Consider a neural network having one neuron in the input layer, two hidden layers each with one neuron and one neuron in the output layer (see figure \ref{fig:sim_NN}). 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.5\textwidth]{Figures/simpNN.png}
    \caption{A simple neural network with only neuron in each layer}
    \label{fig:sim_NN}
\end{figure} 
We can write the loss function as 
$$C = \frac{1}{N} \sum_{k=0}^{N-1} C_k.$$
Let us focus on the cost correponding to one training example,
$$C_0 = (a^{(L)} - y)^2$$ 
where $a^{(L)}$ is the network output and $y$ is the desired output. We label the activation/output from the last neuron with a superscript $L$, indicating which layer it's in, so the
activation of the previous neuron becomes $a^{(L-1)}$. We can express the last activation $a^{(L)}$ by 
$$a^{(L)} = \sigma (w^{(L)}a^{(L-1)} + b^{(L)})$$
where $w^{(L)}$ and $b^{(L)}$ denote the weight and bias of the last neuron respectively. We introduce a new variable $z$ (called weighted sum) for the input to the activation function and keep the same superscript as the activation for labeling it:
$$z^{(L)} = w^{(L)} a^{(L-1)} + b^{(L)}$$
$$a^{(L)} = \sigma (z^{(L)}) $$
We first calculate $\frac{\partial C_0}{\partial w^{(L)}}$ which gives us a measure of how changes in $w^{(L)}$ affects $C_0$. We use the chain-rule from Calculus and write, 
\begin{equation}
    \label{eq:sim_grad_w}
    \frac{\partial C_0}{\partial w^{(L)}} = \frac{\partial z^{(L)}}{\partial w^{(L)}} \frac{\partial a^{(L)}}{\partial z^{(L)}} \frac{\partial C_0}{\partial a^{(L)}}
\end{equation}
The terms in equation \eqref{eq:sim_grad_w} become clear if we refer to the tree of variables of the network (figure \ref{fig:tree_NN}). We can express the terms in equation \eqref{eq:sim_grad_w} by
\begin{equation*}
    \begin{aligned}
        & \frac{\partial z^{(L)}}{w^{(L)}} = a^{(L-1)} \quad \text{since} \ z^{(L)} = w^{(L)} a^{(L-1)} + b^{(L)}\\
        & \frac{\partial a^{(L)}}{\partial z^{(L)}} = \sigma'(z^{(L)}) \quad \text{since} \ a^{(L)} = \sigma (z^{(L)})\\
        & \frac{\partial C_0}{\partial a^{(L)}} = 2 (a^{(L)} - y) \quad \text{since} \ C_0 = (a^{(L)} - y)^2.
    \end{aligned}
\end{equation*}
where $\sigma'(z^{(L)})$ denotes the derivative of the activation function. This reduces equation \eqref{eq:sim_grad_w} to 
\begin{equation}
    \label{eq:del_w}
    \frac{\partial C_0}{\partial w^{(L)}} = a^{(L-1)} \cdot \sigma'(z^{(L)}) \cdot 2(a^{(L)} -y). 
\end{equation}
\begin{figure}[htbp]
    \centering
        \includegraphics[width=.5\textwidth]{Figures/treeNN.png}
        \caption{This tree shows how the variables are connected in the neural network}
        \label{fig:tree_NN}
    \end{figure} 
Next, we calculate the gradient of loss with respect to bias, $b^{(L)}$. 
$$\frac{\partial C_0}{\partial b^{(L)}} = \frac{\partial z^{(L)}}{\partial b^{(L)}} \frac{\partial a^{(L)}}{\partial z^{(L)}} \frac{\partial C_0}{\partial a^{(L)}}$$
We need to just calculate $\frac{\partial z^{(L)}}{\partial b^{(L)}}$ in order to find $\frac{\partial C_0}{b^{(L)}}$ since the other terms are already available to us from equation \eqref{eq:sim_grad_w}. Therefore, we can write, 
\begin{equation}
    \label{del_b}
    \frac{\partial C_0}{\partial b^{(L)}} = 1 \cdot \sigma'(z^{(L)}) \cdot 2(a^{(L)} -y).
\end{equation}
At last, we calculate $\frac{\partial C_0}{\partial a^{(L-1)}}$. 
\begin{equation}
    \label{eq:del_a}
    \frac{\partial C_0}{\partial a^{(L-1)}} = \frac{\partial z^{(L)}}{\partial a^{(L-1)}} \frac{\partial a^{(L)}}{\partial z^{(L)}} \frac{\partial C_0}{\partial a^{(L)}} = w^{(L)}\cdot\sigma'(z^{(L)}) \cdot 2(a^{(L)} -y).
\end{equation}
The procedure to obtain the cost gradient for second-last layer variables is pretty straightforward now. We just replace the superscript $L$ by $L-1$. 
\begin{equation}
    \label{eq:del_sl}
    \begin{aligned}
        &\frac{\partial C_0}{\partial w^{(L-1)}} = \frac{\partial z^{(L-1)}}{\partial w^{(L-1)}} \frac{\partial a^{(L-1)}}{\partial z^{(L-1)}} \frac{\partial C_0}{\partial a^{(L-1)}}\\
        &\frac{\partial C_0}{\partial b^{(L-1)}} = \frac{\partial z^{(L-1)}}{\partial b^{(L-1)}} \frac{\partial a^{(L-1)}}{\partial z^{(L-1)}} \frac{\partial C_0}{\partial a^{(L-1)}}
    \end{aligned}
\end{equation}
We have already calculated $\frac{\partial C_0}{\partial a^{(L-1)}}$ in the previous step. For the remaining terms, we know that,
 $\frac{\partial z^{(L-1)}}{\partial w^{(L-1)}} = a^{(L-2)}$, $\frac{\partial a^{(L-1)}}{\partial z^{(L-1)}} = \sigma'(z^{(L-1)})$ and  $\frac{\partial z^{(L-1)}}{\partial b^{(L-1)}} = 1$
 thus reducing the equations \eqref{eq:del_sl} to
 \begin{equation}
    \label{eq:del_sl_f}
    \begin{aligned}
        &\frac{\partial C_0}{\partial w^{(L-1)}} =  a^{(L-2)} \cdot \sigma'(z^{(L-1)}) \cdot \frac{\partial C_0}{\partial a^{(L-1)}}\\
        &\frac{\partial C_0}{\partial b^{(L-1)}} = 1 \cdot \sigma'(z^{(L-1)}) \cdot \frac{\partial C_0}{\partial a^{(L-1)}}
    \end{aligned}
\end{equation}
We have now finished the calculation of all gradients of the network \ref{fig:sim_NN}. Although the above procedure is described for a network with only two hidden layers, 
we can easily generalize the methodology for networks with more hidden layers. In fact, the procedure is essentially the same even if we increase the number of neurons
in each layer. We demonstrate this by considering a general neural network with multiple hidden layers each having multiple neurons (see figure \ref{fig:back_p}). 
\begin{figure}[htbp]
    \centering
        \includegraphics[width=.5\textwidth, angle = -90]{Figures/back_prop.jpg}
        \caption{Figure shows the notations assigned to different variables in a neural network while implementing backpropagation}
        \label{fig:back_p}
    \end{figure} 
We assign the following notations to weights, biases, weighted sums and activations of the network:
\begin{itemize}
    \item The activations are denoted by $a_j^{(l)}$ where the superscript represents the layer number and subscript represents the neuron number. The 
    layers are numbered starting from $l=1$ (the first layer i.e. input layer) while the neurons are numbered starting from $j=0$ (the first neuron).
    \item The weight for the connection between $k$th neuron of layer $l-1$ and $j$th neuron of layer $l$ is denoted by $w_{jk}^{(l)}$. The bias for the $j$th neuron in
    layer $l$ is denoted by $b_j^{(l)}$.
    \item The weighted sums are denoted by $z_j^{(l)}$ which are passed to an activation function resulting in activation $a_j^{(l)}$.
\end{itemize}
We have the following expressions for the cost function (for one training example), activations and weighted sums:
$$C_0 = \sum_{j=0}^{n_L -1} (a_j^{(L)} - y_j)^2, \quad a_j^{(l)} = \sigma(z_j^{(l)}), \quad z_j^{(l)} = \sum_{k=0}^{n_{l-1} -1} w_{jk}^{(l)}a_k^{(l-1)} + b_j^{(l)}$$
We can now derive the gradients following the chain-rule. We start by first calculating the gradients for the last layer,
\begin{equation}
    \label{eq:bp_eq_d}
    \begin{aligned}
        &\frac{\partial C_0}{\partial w_{jk}^{(L)}} = \frac{\partial z_j^{(L)}}{\partial w_{jk}^{(L)}} \frac{\partial a_j^{(L)}}{\partial z_j^{(L)}}\frac{\partial C_0}{\partial a_j^{(L)}}\\
        &\frac{\partial C_0}{\partial b_j^{(L)}} = \frac{\partial z_j^{(L)}}{\partial b_j^{(L)}} \frac{\partial a_j^{(L)}}{\partial z_j^{(L)}}\frac{\partial C_0}{\partial a_j^{(L)}}
    \end{aligned}
\end{equation}
We also derive the gradient with respect to activations in the second-last layer,
$$\frac{\partial C_0}{\partial a_k^{(L-1)}} = \sum_{j=0}^{n_{L} -1} \frac{\partial z_j^{(L)}}{\partial a_k^{(L-1)}} \frac{\partial a_j^{(L)}}{\partial z_j^{(L)}}\frac{\partial C_0}{\partial a_j^{(L)}}$$
We can evaluate the gradients $\frac{\partial C_0}{\partial w_{jk}^{(L-1)}}$ and $\frac{\partial C_0}{\partial b_j^{(L-1)}}$ using $\frac{\partial C_0}{\partial a_j^{(L-1)}}$. In this way we can keep moving backwards in the network (hence the name backpropagation)
and find gradients on the way back.  Notice that the only major change we see in the expressions for gradients when moving to complex neural networks from simple networks is for $\frac{\partial C_0}{\partial a_k^{(L-1)}}$. 
This is attributed to that fact that now we have multiple neurons in each layer. A change in the activation, say $\frac{\partial C_0}{\partial a_k^{(L-1)}}$, can affect the cost through multiple paths.
In general, we can write the derivatives of cost/loss function with respect to weights and biases for neurons in any layer $l$ as,
\begin{equation}
    \label{eq:bp_eq}
    \begin{aligned}
        &\frac{\partial C_0}{\partial w_{jk}^{(l)}} = a_k^{(l-1)}\sigma'(z_j^{(l)}) \frac{\partial C_0}{\partial a_j^{(l)}}\\
        &\frac{\partial C_0}{\partial b_j^{(l)}} = \sigma'(z_j^{(l)}) \frac{\partial C_0}{\partial a_j^{(l)}}
    \end{aligned}
\end{equation}
where 
$$\frac{\partial C_0}{\partial a_k^{(l)}} = \sum_{j=0}^{n_{l+1} -1} w_{jk}^{(l+1)} \sigma'(z_j^{(l+1)}) \frac{\partial C_0}{\partial a_j^{(l+1)}} \quad \text{or} \quad  2(a_k^{(L)} - y_k) \ \text{when} \ l=L.$$
The backpropagation algorithm is generally combined with a learning algorithm like stochastic gradient descent to update weights and biases. Let us see how this works for a mini-batch of $m$ training examples:
\begin{enumerate}
    \item Input a set of training examples. 
    \item For each training example $x$: Set the corresponding input activation $a^{x,1}$ and perform the following steps:
    \begin{itemize}
        \item Feedforward: For each $l = 2,3,....L$, compute $z_j^{(l)} = \sum_{k=0}^{n_{l-1} -1} w_{jk}^{(l)} a_k^{(l-1)} + b_j^{(l)}$ and $a_j^{(l)} = \sigma (z_j^{(l)})$
        \item Compute gradients for the last layer: $$\frac{\partial C_x}{\partial a_j^{(L)}}, \frac{\partial C_x}{\partial w_{jk}^{(L)}}, \frac{\partial C_x}{\partial b_j^{(L)}}$$
        \item Backpropagate: for $l = L-1, L-2, ...., 2$ compute $$\frac{\partial C_x}{\partial a_j^{(l)}}, \frac{\partial C_x}{\partial w_{jk}^{(l)}}, \frac{\partial C_x}{\partial b_j^{(l)}}$$
    \end{itemize}
    \item Gradient descent: for each $l = L, L-1, ......, 2$ update weights and biases
        $$w_{jk}^l \rightarrow w_{jk}^l - \frac{\eta}{m} \sum_x \frac{\partial C_x}{\partial w_{jk}^{(l)}} $$
        $$b_j^l \rightarrow b_j^l - \frac{\eta}{m} \sum_x \frac{\partial C_x}{\partial b_j^{(l)}} $$
\end{enumerate}
\subsection{Why backpropagation is a fast algorithm ?}
In the last section, we have illustrated the procedure to find gradient of the cost function. At first glance, the backpropagation algorithm may
seem complicated since it involves a lot of steps and one has to be very careful about the notations. In reality, the backpropagation algorithm is very 
simple to implement and is considered one of the most efficient methods for computing gradients. Most of the terms like $a_k^{(l-1)}, w_{jk}^{(l+1)}$, in 
the expressions for gradients are already available to the network when making a forward pass; the rest, for example $\sigma'(z_j^{(l)})$ can be easily computed.
We can see the advantages of using backpropagation when we compare it to another popular method for calculating gradients. Let us denote (for simplicity) the cost function by $C(w,b)$ 
where $w$ are the weights $w_1,w_2,.....$ and $b$ are the biases $b_1,b_2,....$ of a network. We can use the fundamental definition of partial derivatives to approximate gradients,
\begin{equation}
    \label{eq:part_de}
    \frac{\partial C}{\partial w_j} \approx \frac{C(w + \epsilon e_j, b) - C(w,b)}{\epsilon}
\end{equation}
where $\epsilon > 0$ is a small positive number, and $e_j$ is a unit-vector in the $j-$th direction. In other words, we can compute $\frac{\partial C}{\partial w_j}$ by first 
calculating the cost $C$ for two slightly  different values of $w_j$ and then applying equation \eqref{eq:part_de}. We can use this approach to 
calculate derivatives with respect to all weight and biases of the network. This method is extremly easy to implement and certainly looks better than the chain-rule. Therefore, one
might be tempted to use this instead of backpropagation. Unfortunately, this method is extremely slow ! 

Suppose we have a million ($10^6$) weights and biases in a network. In order to get 
gradients using equation \eqref{eq:part_de}, we have to compute the cost $10^6 + 1$ times; once to compute $C(w)$ and $10^6$ times to calculate $C(w + \epsilon e_j, b)$ for different $j$'s. This is equivalent to making
$10^6 +1$ forward passes of the network. On the other hand, with backpropagation, we need to make just one forward pass and one backward pass to calculate all the gradients. The computational cost 
of the backward pass is similar to the forward pass since both involve similar computations. Therefore, rougly speaking, (for this example) the backpropagation method is a million times faster than the numerical gradient computation method (equation \eqref{eq:part_de}).
The backpropagation is also more accurate than the numerical gradient method. The expression for gradient used in equation \eqref{eq:part_de} is an approximation of gradients and depends on the value of $\epsilon$ whereas the expressions used in backpropagation are exact.
\subsection{Implementation of a neural network to classify digits}
We use the ideas discussed in the previous sections to write a code for implementing a neural network
which can classify hand-written digits. A jupyter notebook which illustrates how this neural network performs on MNIST test data set is available in \complementary{\theexample}.
%%%%%%%%%%% UAT %%%%%%%%%%%%%%%%%%%%%
\section{The universal approximation theorem}
Neural networks are mathematical functions. We use training data to look for suitable weights and biases
such that the neural network can map/obtain the underlying function from which the data is drawn. But, how can one
believe that a neural network have the capability of finding the underlying function. What is so special about the
neural network structure that after training it is able to give us great results on unseen data/ test data ? The answer to these
questions lies in the universal approximation theorem (UAT) proposed by George Cybenko in $1989$. 
George Cybenko gave a rigorous proof that feed-forward neural networks can approximate any continuous function defined on a compact set \cite{cybenko1989approximation}. To understand the 
proof of this theorem, one should be familiar with some concepts from measure theory and functional analysis. 
Let us first briefly go through the mathematical preliminaries.
\begin{itemize}
    \item Consier an $n$-dimensional unit cube, $I_n = [0,1]^n$. In the universal approximation theorem, the continuous
    functions that a neural network aims to approximate are defined on compacts sets, $I_n$ i.e. the domain of continuous
    functions is $I_n$. 
    \item Let $C(I_n)$ denote the space of continuous functions on $I_n$.
    For any function $f \in C(I_n), f : [0,1]^n \rightarrow \mathbb{R}$, one can define a suitable norm, 
    $\|f\| = \text{sup}_{x \in I_n} |f(x)|$ known as uniform/supremum norm. We can associate a dual space to $C(I_n)$ 
    defined as $C(I_n)^* = \{L : C(I_n) \rightarrow \mathbb{R}\}$ where $L$ is a linear operator \cite{kreyszig1991introductory}.
    \item UAT is defined for feed-forward neural networks having an input layer (with multiple neurons), one hidden layer and
    an output layer with only one neuron (see figure \ref{fig:ffnn_UAT}). Therefore, the functions generated by the neural network can be described as
    $$G(x) = \sum_{j=1}^{N} \alpha_j \sigma (w_j\cdot x + b_j), \quad w_j \in \mathbb{R}^n, \alpha_j, b_j \in \mathbb{R}, x \in I_n$$
    where $N$ is the number of neurons in the hidden layer. We can note that bias is not added to output from the hidden
    layer. Also the output is not passed to an activation function. We denote the set of functions of the form $G(x)$
    by $\mathcal{N}$. In the problem of hand-written digit recognition, we assumed that the activation function, $\sigma$ is logistic/sigmoid. 
    However, UAT was proved for a general class of activation functions. The actvation functions are assumed to be continuous and 
    discriminatory \cite{cybenko1989approximation}. 
\end{itemize} 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.4\textwidth]{Figures/ffnn_UAT.jpg}
    \caption{A feed-forward neural network with an input layer (having multiple neurons), one hidden layer and an output layer (having only one neuron).}
    \label{fig:ffnn_UAT}
\end{figure}
\begin{definition}[Discriminatory function \cite{cybenko1989approximation}]
    We say that a function $\sigma$ is discriminatory if for a measure $\mu \in M(I_n)$ (where $M(I_n)$ is the space 
of finite, signed regular Borel measures on $I_n$) 
$$\int_{I_n} \sigma (\mathbf{w}\cdot\mathbf{x} + b) \ d\mu(\mathbf{x}) = 0$$
for all $\mathbf{w} \in \mathbb{R}^n$ and $b\in \mathbb{R}$ implies that $\mu = 0$.
\end{definition}
\begin{thm}[Universal approximation theorem \cite{cybenko1989approximation}]
    \label{thm:UAT}
    Let $\sigma$ be any continuous discriminatory function. Then the set $\mathcal{N}$ of functions of the form 
    $$G(x) = \sum_{j=1}^{N} \alpha_j \sigma (w_j\cdot x + b_j), \quad w_j \in \mathbb{R}^n, \alpha_j, b_j \in \mathbb{R}, x \in I_n$$
    are dense in $C(I_n)$. In other words, given any function $f \in C(I_n)$ and $\epsilon > 0$, there exists
    a $G(x) \in \mathcal{N}$ such that 
    $$\| G(x) - f(x)\| < \epsilon \ \text{for all} \ x \in I_n$$
\end{thm}
Before going into the proof of UAT, we will familiarize ourselves with some key ideas from measure theory \cite{tao2011introduction}, \cite{kyleUAT}.
\begin{itemize}
    \item Our aim to measure subsets of $I_n = [0,1]^n$. A collection $\sum$ of subsets of $I_n$ is called a 
    $\sigma$-algebra (note that this $\sigma$ has no relation to activation function $\sigma$)if 
    \begin{enumerate}
        \item $I_n \in \sum$
        \item If $A \in \sum$, then $A^c = I_n\backslash A \in \sum$
        \item If $A_1, A_2, .....$ is a countable collection of subsets of $\sum$ then $\bigcup^\infty A_i \in \sum$.
    \end{enumerate}
    \item Borel $\sigma$-algebra on $I_n$ is the smallest $\sigma$-algebra  containing all open sets in $I_n$. Sets in Borel $\sigma$-algebra 
    are called Borel sets.
    \item A finite signed Borel measure $\mu$ on $I_n$ is a real valued  function, $\mu : \sum \rightarrow \mathbb{R}$  (where
    $\sum$ is a Borel $\sigma$-algebra defined on $I_n$) such that 
    \begin{enumerate}
        \item $\mu (\phi) = 0$
        \item $\mu (\bigcup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} \mu(A_i), A_i \ \text{are disjoint sets}$
        \item $\mu(A_i) < \infty$
    \end{enumerate}
    \item A Borel measure is regular if 
    \begin{align*}
        &\mu(A) = \text{sup} \{\mu(K) | K \subseteq A, K \ \text{is compact}\} \\
        &\mu(A) = \text{inf} \{\mu(U) | A \subseteq U, U \ \text{is open}\}
    \end{align*}
    In other words, what regular means is every measurable set can be approximated from above by open measurable 
    sets and from below by compact measurable sets. 
    \item We define $M(I_n)$ as the set of all finite signed regular Borel measures on $I_n$.
\end{itemize}
The proof of UAT also uses ideas from Hahn-Banach theorem and Riesz representation theorem. These theorems 
have various equivalent versions. We now define the versions which we will refer to later in the proof. 
\begin{thm}[Riesz representation theorem \cite{rudin}]
    \label{thm:RR}
    If $K \subset \mathbb{R}^d$ is a compact set, then every linear functional $L$ defined on
    $C(K)$ can represented by a unique regular signed Borel measure $\mu \in M(K)$ in the sense that,
    $$L(f) = \int_K f \ d\mu$$
    where $f: K \rightarrow \mathbb{R}$.
\end{thm}
\begin{thm}[Hahn-Banach theorem, \cite{rudin}]
    \label{thm:HB}
    Let $M$ be a linear subspace of a normed linear space $X$, and $f_0 \in X$. Then $f_0$ is 
    in the closure $\overline{M}$ of $M$ if and only if there is no bounded linear functional 
    $L$ on $X$ such that $L(f) = 0 $ for all $f\in M$ but $L(f_0) \neq 0$.
\end{thm}
We now give the proof of universal approximation theorem \ref{thm:UAT}.
\begin{proof}
   $C(I_n)$ is a vector space equipped with a supremum norm and hence is a normed vector space. It is
   clear that the set formed by functions of the form $G(x)$ is a subset of $C(I_n)$ i.e. $\mathcal{N} \subset C(I_n)$.
   We claim that $\mathcal{N}$ is also a linear subspace of $C(I_n)$. This is true due to the fact that if 
   we take two functions $G_1(x), G_2(x) \in \mathcal{N}$, we can always find a function $G_3(x) \in \mathcal{N}$ such that 
   $G_3(x) = \alpha G_1(x) + \beta G_2(x)$ for all $\alpha, \beta \in \mathbb{R}$. Our next claim is that the closure of 
   $\mathcal{N}$ is all of $C(I_n)$ i.e. $\mathcal{N}$ is dense in $C(I_n)$. We prove this by contradiction \cite{rudin}. 

   Assume that the closure of $\mathcal{N}$, $\overline{\mathcal{N}}$ is not all of $C(I_n)$ i.e. 
   $\overline{\mathcal{N}} \neq C(I_n)$. It can be proved (refer any standard functional analysis book) that $\overline{\mathcal{N}}$
   is a closed proper subspace of $C(I_n)$.  By Hahn Banach theorem \ref{thm:HB}, there exists a bounded 
   linear functional on $C(I_n)$, let's call it $L$, with the property that $L \neq 0$ but $L(\overline{\mathcal{N}}) = L(\mathcal{N}) = 0$.
   By following Riesz representation theorem \ref{thm:RR}, we can write this linear operator, $L$ as 
   $$L(h) = \int_{I_n} h(x) \ d\mu(x)$$
   for some $\mu \in M(I_n) \ \forall h \in C(I_n)$. Since this operator returns zero for all elements of $\mathcal{N}$,
   we must have for all $w \in \mathbb{R}^n, b \in \mathbb{R}$ 
   $$\int_{I_n} \sigma (w \cdot x + b) \ d\mu(x) = 0.$$
   However, we assumed that $\sigma$ is a discriminatory functions therefore this condition implies that $\mu =0$. This contradicts our initial assumption that 
   $\overline{\mathcal{N}} \neq C(I_n)$ which led to $L \neq 0 \equiv \mu \neq 0$ via Hahn Banach theorem. Hence, the subspace $\mathcal{N}$ must be dense in $C(I_n)$. 
\end{proof}
The universal approximation theorem gives us confidence that neural networks can indeed approximate any continuous function 
provided that the activation function is continuous and discriminatory. However, it is not clear how to build activation functions which are 
discriminatory.
\begin{definition}
    \label{def:sig}
    We say that $\sigma$ is sigmoidal if 
    \begin{equation*}
        \sigma(t) \rightarrow 
         \begin{cases}
           1 &\quad \text{as} \ \ t\rightarrow +\infty\\
           0 &\quad \text{as} \ \ t\rightarrow -\infty.
         \end{cases}
    \end{equation*}
\end{definition}
\begin{lemma}
    \label{lem:sig}
    Any continuous sigmoidal function is discriminatory.
\end{lemma}
\begin{proof}
    Refer \cite{cybenko1989approximation}.
\end{proof}
The lemma \ref{lem:sig} gives us a way to build suitable activation functions. In fact, the sigmoid/logistic function we used as an activation function
in the neural network for solving hand-written recognition problem satisfies the conditions of definition \ref{def:sig} and is therefore, discriminatory. Although 
logistic function is monotonically increasing, no monotonicity is required by definition \ref{def:sig}.