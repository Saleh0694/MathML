{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "387810fc-4e84-4f7e-8cb2-450a79f91e08",
   "metadata": {},
   "source": [
    "# MNIST dataset - Classifiying hand-written digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35646e6a",
   "metadata": {},
   "source": [
    "Let's start by loading libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fda02c12-6b7c-4cbf-b3b6-430e4fdd389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp \n",
    "import flax.linen as nn \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optax\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bebc967-ea8a-4d20-82cb-c31b2b1ae02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "digits = load_digits()\n",
    "Images = digits.data\n",
    "target = digits.target\n",
    "#visualize\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a53b7c-3f2d-4543-b79d-873d3ad58b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer Perceptron\n",
    "    \n",
    "    This class represents a Multi-layer Perceptron (MLP) model, which is a type of feedforward neural network.\n",
    "    MLPs consist of multiple layers of interconnected nodes (called neurons) and are commonly used for\n",
    "    classification tasks.\n",
    "    \n",
    "    Parameters:\n",
    "        nhidden_units (int): Number of hidden units in each layer.\n",
    "        nlayers (int, optional): Number of layers in the MLP. Defaults to 1.\n",
    "        nclasses (int, optional): Number of output classes (categories). Defaults to 10.\n",
    "    \n",
    "    Attributes:\n",
    "        layers (list): List of Dense layers representing the hidden layers in the MLP.\n",
    "        final_layer (Dense): Final Dense layer for producing the output of the MLP.\n",
    "    \"\"\"\n",
    "    nhidden_units: int\n",
    "    nlayers = 1\n",
    "    nclasses = 10\n",
    "    \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Setup the MLP model by creating the hidden layers and the final output layer.\n",
    "        \"\"\"\n",
    "        self.layers = [nn.Dense(self.nhidden_units) for n in range(self.nlayers)]\n",
    "        self.final_layer = nn.Dense(self.nclasses)\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the MLP.\n",
    "        \n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): Input data.\n",
    "            \n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: Output of the MLP.\n",
    "        \"\"\"\n",
    "        for k, lyr in enumerate(self.layers):\n",
    "            x = lyr(x)\n",
    "            x = nn.relu(x)\n",
    "        x = self.final_layer(x)\n",
    "        x = nn.activation.softmax(x)\n",
    "        return x\n",
    "# initializing the optimizer\n",
    "learning_rate = 1e-3\n",
    "optx = optax.adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902b185-9ca8-491f-b8bf-ddee2df6b090",
   "metadata": {},
   "source": [
    "The target values we loaded are integers. We now write a code to one-hot encode them. For example, the category \"1\" would become [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5540928-b997-4439-b46e-20a38a8b4b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target value: 8, corresponding one-hot vector: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encoder(x):\n",
    "    \"\"\"\n",
    "    One-Hot Encoder\n",
    "    \n",
    "    Converts an array of categorical labels to one-hot encoded representation.\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Array of categorical labels.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: One-hot encoded representation of the input labels.\n",
    "    \"\"\"\n",
    "    nclasses = 10  # Number of classes/categories\n",
    "    out = np.zeros((x.shape[0], nclasses))  # Initialize an array of zeros with shape (number of samples, number of classes)\n",
    "    \n",
    "    for i, x_ in enumerate(x):\n",
    "        out[i, x_] = 1.  # Set the corresponding class index to 1 in each sample\n",
    "        \n",
    "    return out\n",
    "target_oh = one_hot_encoder(target)\n",
    "# test\n",
    "print(f\"target value: {target[40]}, corresponding one-hot vector: {target_oh[40,:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a468aa-d570-4492-ab8f-fc8deed40bc9",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab7ee2f-1bea-4479-9641-7c2d0b674fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss function: 7.451040744781494\n",
      "epoch: 100, loss function: 0.03282686322927475\n",
      "epoch: 200, loss function: 0.007416482083499432\n",
      "epoch: 300, loss function: 0.003187159076333046\n",
      "epoch: 400, loss function: 0.0017350796842947602\n",
      "epoch: 500, loss function: 0.0010891821002587676\n",
      "epoch: 600, loss function: 0.0007527557318098843\n",
      "epoch: 700, loss function: 0.0005526680615730584\n",
      "epoch: 800, loss function: 0.0004230765625834465\n",
      "epoch: 900, loss function: 0.000334318436216563\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(Images, target_oh, test_size=0.33, random_state=42)\n",
    "# initialize the neural network \n",
    "model = MLP(256)\n",
    "params = model.init(jax.random.PRNGKey(0), x_test)\n",
    "opt_state = optx.init(params)\n",
    "\n",
    "# define the loss function \n",
    "@jax.jit\n",
    "def cat_cross_entropy(params, x, y_true):\n",
    "    \"\"\"\n",
    "    Categorical Cross Entropy Loss Function\n",
    "\n",
    "    This function calculates the categorical cross entropy loss between the predicted\n",
    "    probabilities and the true one-hot encoded labels.\n",
    "\n",
    "    Parameters:\n",
    "        params (jax.interpreters.xla.DeviceArray): Model parameters.\n",
    "        x (jax.interpreters.xla.DeviceArray): Input data.\n",
    "        y_true (jax.interpreters.xla.DeviceArray): True one-hot encoded labels.\n",
    "\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: Mean categorical cross entropy loss.\n",
    "    \"\"\"\n",
    "    # Forward pass to obtain predicted probabilities\n",
    "    y_pred = model.apply(params, x)\n",
    "\n",
    "    # Calculation of the loss per example\n",
    "    loss_per_example = jnp.sum(-y_true * jnp.log(y_pred), axis=1)\n",
    "\n",
    "    # Return the mean loss across all examples\n",
    "    return jnp.mean(loss_per_example)\n",
    "\n",
    "loss_fn = cat_cross_entropy\n",
    "loss_grad_fn = jax.value_and_grad(cat_cross_entropy) # a function to evaluate the function and its gradient)\n",
    "\n",
    "# training loop\n",
    "n_epochs = 1000 #number of training epochs \n",
    "for e in range(n_epochs):\n",
    "    loss_val, grad = loss_grad_fn(params, x_train, y_train)\n",
    "    updates, opt_state = optx.update(grad, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if e % 100 == 0:\n",
    "        print(f\"epoch: {e}, loss function: {loss_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e61faf-3a3d-4abb-9edc-954241271303",
   "metadata": {},
   "source": [
    "Test the learned function on some random input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbca75e5-2ec6-4584-b5ba-195279d6fdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ture value: 5 predicted: 5\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.randint(low=0, high=x_test.shape[0], size=(1,))\n",
    "y_pred = np.argmax(model.apply(params, x_test[sample,:]))\n",
    "print(f\"ture value: {np.argmax(y_test[sample,:])} predicted: {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352a57e-0e56-47b1-b514-2249afd66b2f",
   "metadata": {},
   "source": [
    "Evaluate the training and test errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "497b3640-7462-47fd-b1cd-c5e5e5dde919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.00027058576233685017, test loss: 0.09268166869878769\n"
     ]
    }
   ],
   "source": [
    "tr_loss = cat_cross_entropy(params, x_train, y_train)\n",
    "te_loss = cat_cross_entropy(params, x_test, y_test)\n",
    "print(f\"training loss: {tr_loss}, test loss: {te_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256a74b-3c1e-45fd-81e7-d4f890756116",
   "metadata": {},
   "source": [
    "We now evaluate the training and test error. Does the model suffer from overfitting? from underfitting? from both?\n",
    "\n",
    "Experiment with the following parameters and see how that affects over-/underfitting. \n",
    "\n",
    "- Complexity of the neural network; by increasing the number of hidden units or number of layers.\n",
    "- Add a regularization layer; it prevents the weights of the neural network from becoming so big. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
